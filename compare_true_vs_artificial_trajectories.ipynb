{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare true vs artificial trajectories\n",
    "\n",
    "This notebook compares the fetaures from the true and artificial trajectories, performing a cell-to-cell matching based on their initial position.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import re\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from typing import Any, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from numpy.typing import ArrayLike\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use([\"ggplot\", \"fast\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CWD if using non-local venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/workspaces/biocomp/tboyer/sources/CellProfiler_GaussianProxy_analyses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True trajectrories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_features = pd.read_parquet(\n",
    "    Path(\n",
    "        \"analyses\",\n",
    "        \"biotine_resized\",\n",
    "        \"features_through_time_of_full_lifetime_cells.parquet\",\n",
    "    )\n",
    ")\n",
    "true_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing trajectories of: filtered nucleus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select nucleus features\n",
    "true_features = true_features[true_features[\"file\"] == \"filtered_nucleus\"]\n",
    "true_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generated trajectories\n",
    "\n",
    "Load, add `global_object_id`, and concat\n",
    "\n",
    "**beware: there is no (trivial) matching between `global_object_id` in true and generated data!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_base_path_path = Path(\n",
    "    \"/\",\n",
    "    \"projects\",\n",
    "    \"static2dynamic\",\n",
    "    \"Thomas\",\n",
    "    \"experiments\",\n",
    "    \"GaussianProxy\",\n",
    "    \"biotine_all_paired_new_jz_MANUAL_WEIGHTS_DOWNLOAD_FROM_JZ_11-02-2025_14h31\",\n",
    "    \"inferences\",\n",
    ")\n",
    "\n",
    "# `plate_names` and `experiments_names` must be in 1-1 ordered correspondence!\n",
    "experiments_names = [\n",
    "    \"InvertedRegeneration_100_diffsteps_no_SNR_leading_A_13_fld_2\",\n",
    "    \"InvertedRegeneration_100_diffsteps_no_SNR_leading_f32_B_13_fld_4\",\n",
    "    \"InvertedRegeneration_100_diffsteps_no_SNR_leading_C_13_fld_3\",\n",
    "    \"InvertedRegeneration_100_diffsteps_no_SNR_leading_D_14_fld_1\",\n",
    "    \"InvertedRegeneration_100_diffsteps_no_SNR_leading_f32_E_14_fld_1\",\n",
    "    \"InvertedRegeneration_100_diffsteps_no_SNR_leading_F_14_fld_4\",\n",
    "    \"InvertedRegeneration_100_diffsteps_no_SNR_leading_G_13_fld_1\",\n",
    "    \"InvertedRegeneration_100_diffsteps_no_SNR_leading_H_14_fld_2\",\n",
    "    \"InvertedRegeneration_100_diffsteps_no_SNR_leading_I_14_fld_3\",\n",
    "    # \"InvertedRegeneration_100_diffsteps_no_SNR_leading_J_14_fld_2\", # TODO: run the CP pipeline!\n",
    "    \"InvertedRegeneration_100_diffsteps_no_SNR_leading_K_14_fld_1\",\n",
    "    # \"InvertedRegeneration_100_diffsteps_no_SNR_leading_L_13_fld_2\", # TODO: run the CP pipeline!\n",
    "    \"InvertedRegeneration_100_diffsteps_f32_noSNR_leading_M_13_fld_3\",\n",
    "    # \"InvertedRegeneration_100_diffsteps_no_SNR_leading_N_14_fld_1\", # TODO: run the CP pipeline!\n",
    "    # \"InvertedRegeneration_100_diffsteps_no_SNR_leading_O_14_fld_4\", # TODO: run the CP pipeline!\n",
    "]\n",
    "\n",
    "plate_names = [\n",
    "    \"A_13_fld_2\",\n",
    "    \"B_13_fld_4\",\n",
    "    \"C_13_fld_3\",\n",
    "    \"D_14_fld_1\",\n",
    "    \"E_14_fld_1\",\n",
    "    \"F_14_fld_4\",\n",
    "    \"G_13_fld_1\",\n",
    "    \"H_14_fld_2\",\n",
    "    \"I_14_fld_3\",\n",
    "    # \"J_14_fld_2\",\n",
    "    \"K_14_fld_1\",\n",
    "    # \"L_13_fld_2\",\n",
    "    \"M_13_fld_3\",\n",
    "    # \"N_14_fld_1\",\n",
    "    # \"O_14_fld_4\",\n",
    "]\n",
    "assert len(experiments_names) == len(plate_names), \"Number of experiments and plates must match!\"\n",
    "assert len(set(plate_names)) == len(plate_names), \"Plate names must be unique!\"\n",
    "for idx in range(len(plate_names)):\n",
    "    assert experiments_names[idx].endswith(plate_names[idx]), (\n",
    "        f\"Experiment name {experiments_names[idx]} does not end with plate name {plate_names[idx]}!\"\n",
    "    )\n",
    "\n",
    "experiments_paths = [\n",
    "    experiments_base_path_path / name / \"trajectories_-1_1 raw\" / \"cp_analysis\"\n",
    "    for name in experiments_names\n",
    "]\n",
    "experiments_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load gen data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_features_list = []\n",
    "\n",
    "for exp_idx, exp_p in enumerate(experiments_paths):\n",
    "    this_plate_gen_features = pd.read_csv(exp_p / \"filtered_nucleus.csv\")\n",
    "\n",
    "    this_plate_gen_features[\"plate_name\"] = plate_names[exp_idx]\n",
    "\n",
    "    this_plate_gen_features[\"global_object_id\"] = (\n",
    "        plate_names[exp_idx] + \"-\" + this_plate_gen_features[\"TrackObjects_Label_10\"].astype(str)\n",
    "    )\n",
    "\n",
    "    gen_features_list.append(this_plate_gen_features)\n",
    "\n",
    "gen_features = pd.concat(gen_features_list, ignore_index=True)\n",
    "gen_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found {gen_features['global_object_id'].nunique()} objects in the synthetic data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add time info to artificial frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_features[\"time\"] = (\n",
    "    gen_features[\"FileName_images\"].str.extract(r\"^frame_(\\d+).tiff$\").astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check time data concistency\n",
    "gen_features[[\"time\", \"FileName_images\"]].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter true traj on selected plate only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "these_plates_true_features = true_features[\n",
    "    true_features[\"global_object_id\"].str.startswith(tuple(plate_names))\n",
    "].copy(deep=True)\n",
    "# deep clone for in-place modif later\n",
    "\n",
    "these_plates_true_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found {these_plates_true_features['global_object_id'].nunique()} objects in the true data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get initial positions of true cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: only one starting pos per object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_features_mask_time_1 = gen_features[\"time\"] == 0  # beware, zero here is true time 1!\n",
    "gen_features_time_1 = gen_features[gen_features_mask_time_1]\n",
    "gen_features_time_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_1_mask = these_plates_true_features[\"Metadata_time\"] == 1\n",
    "these_plates_true_features_time_1 = these_plates_true_features[time_1_mask]\n",
    "assert (\n",
    "    len(these_plates_true_features_time_1)\n",
    "    == these_plates_true_features[\"global_object_id\"].nunique()\n",
    ")\n",
    "\n",
    "# used later for pairing\n",
    "initial_true_positions_object_ids_to_pos = {}\n",
    "\n",
    "for obj_id in tqdm(these_plates_true_features_time_1[\"global_object_id\"].unique()):\n",
    "    this_obj_mask = these_plates_true_features_time_1[\"global_object_id\"] == obj_id\n",
    "    this_obj_init_pos = these_plates_true_features_time_1.loc[\n",
    "        this_obj_mask, (\"AreaShape_Center_X\", \"AreaShape_Center_Y\")\n",
    "    ].values\n",
    "    assert len(this_obj_init_pos) == 1, (\n",
    "        f\"Found more than one initial position for object {obj_id}: {this_obj_init_pos}\"\n",
    "    )\n",
    "    initial_true_positions_object_ids_to_pos[obj_id] = this_obj_init_pos[0]\n",
    "\n",
    "for obj_id in tqdm(gen_features_time_1[\"global_object_id\"].unique()):\n",
    "    this_obj_mask = gen_features_time_1[\"global_object_id\"] == obj_id\n",
    "    this_obj_init_pos = gen_features_time_1.loc[\n",
    "        this_obj_mask, (\"AreaShape_Center_X\", \"AreaShape_Center_Y\")\n",
    "    ].values\n",
    "    assert len(this_obj_init_pos) == 1, (\n",
    "        f\"Found more than one initial position for object {obj_id}: {this_obj_init_pos}\"\n",
    "    )\n",
    "\n",
    "initial_true_positions_object_ids_to_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the L2 distance matrix between all true center and all generated centers\n",
    "\n",
    "Not forgetting to take into account image size..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_IMAGE_SIZE = 2040\n",
    "GEN_IMAGE_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the true images are resized to gen dim at the very beginning of the CP pipeline so GEN_IMAGE_SIZE for both here!\n",
    "initial_true_positions_arrays: \"dict[str, dict[str, np.ndarray]]\" = {}  # dict[plate name -> dict[object id -> position]]\n",
    "initial_gen_positions_arrays: \"dict[str, dict[str, np.ndarray]]\" = {}  # dict[plate name -> dict[object id -> position]]\n",
    "\n",
    "# ! true images are resized to gen dim at the very beginning of the CP pipeline so GEN_IMAGE_SIZE for both here !\n",
    "\n",
    "for plate_name in plate_names:\n",
    "    # true positions\n",
    "    this_plate_df = these_plates_true_features_time_1.loc[\n",
    "        these_plates_true_features_time_1[\"global_object_id\"].str.startswith(plate_name)\n",
    "    ]\n",
    "    initial_true_positions_arrays[plate_name] = {}\n",
    "    for obj_id in this_plate_df[\"global_object_id\"].unique():\n",
    "        this_obj_mask = this_plate_df[\"global_object_id\"] == obj_id\n",
    "        this_obj_init_pos = this_plate_df.loc[\n",
    "            this_obj_mask, (\"AreaShape_Center_X\", \"AreaShape_Center_Y\")\n",
    "        ].values\n",
    "        assert len(this_obj_init_pos) == 1, (\n",
    "            f\"Found more or less than one initial position for true object {obj_id}: {this_obj_init_pos}\"\n",
    "        )\n",
    "        initial_true_positions_arrays[plate_name][obj_id] = this_obj_init_pos[0] / GEN_IMAGE_SIZE\n",
    "\n",
    "    # gen positions\n",
    "    this_plate_df = gen_features_time_1.loc[\n",
    "        gen_features_time_1[\"global_object_id\"].str.startswith(plate_name)\n",
    "    ]\n",
    "    initial_gen_positions_arrays[plate_name] = {}\n",
    "    for obj_id in this_plate_df[\"global_object_id\"].unique():\n",
    "        this_obj_mask = this_plate_df[\"global_object_id\"] == obj_id\n",
    "        this_obj_init_pos = this_plate_df.loc[\n",
    "            this_obj_mask, (\"AreaShape_Center_X\", \"AreaShape_Center_Y\")\n",
    "        ].values\n",
    "        assert len(this_obj_init_pos) == 1, (\n",
    "            f\"Found more or less than one initial position for gen object {obj_id}: {this_obj_init_pos}\"\n",
    "        )\n",
    "        initial_gen_positions_arrays[plate_name][obj_id] = this_obj_init_pos[0] / GEN_IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the positions as sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_starting_pos(\n",
    "    plate_name: str,\n",
    "    experiments_path: Path,\n",
    "    this_plate_initial_true_positions_flattened_array: np.ndarray,\n",
    "    this_plate_initial_gen_positions_flattened_array: np.ndarray,\n",
    "    circles: bool = False,\n",
    "    threshold: Union[float, None] = None,\n",
    "):\n",
    "    # figures\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    axs[0].imshow(\n",
    "        plt.imread(\n",
    "            f\"/projects/static2dynamic/datasets/biotine/3_channels_min_99_perc_normalized_rgb_stacks_png/{plate_name}_time_01.png\"\n",
    "        ),\n",
    "        alpha=0.7,\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "    axs[0].set_title(\"True starting image\")\n",
    "    gen_img = axs[1].imshow(\n",
    "        plt.imread(\n",
    "            experiments_path.parent / \"frame_00.tiff\",\n",
    "        ),\n",
    "        alpha=0.7,\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "    gen_img.set_extent(np.array(gen_img.get_extent()) * TRUE_IMAGE_SIZE / GEN_IMAGE_SIZE)\n",
    "    axs[1].set_title(\"Synthetic starting image\")\n",
    "    for ax in axs:\n",
    "        if circles:\n",
    "            assert threshold is not None, \"Threshold must be provided if circles are to be drawn\"\n",
    "            for x, y in this_plate_initial_true_positions_flattened_array:\n",
    "                circle = plt.Circle(\n",
    "                    (x * TRUE_IMAGE_SIZE, y * TRUE_IMAGE_SIZE),\n",
    "                    threshold * TRUE_IMAGE_SIZE,\n",
    "                    fill=True,\n",
    "                    color=\"red\",\n",
    "                    alpha=0.5,\n",
    "                )\n",
    "                ax.add_patch(circle)\n",
    "        ax.scatter(\n",
    "            this_plate_initial_gen_positions_flattened_array[:, 0] * TRUE_IMAGE_SIZE,\n",
    "            this_plate_initial_gen_positions_flattened_array[:, 1] * TRUE_IMAGE_SIZE,\n",
    "            label=\"Synthetic\",\n",
    "            s=6,\n",
    "            marker=\"x\",\n",
    "            c=\"blue\",\n",
    "        )\n",
    "        ax.scatter(\n",
    "            this_plate_initial_true_positions_flattened_array[:, 0] * TRUE_IMAGE_SIZE,\n",
    "            this_plate_initial_true_positions_flattened_array[:, 1] * TRUE_IMAGE_SIZE,\n",
    "            label=\"True\",\n",
    "            marker=\"x\",\n",
    "            c=\"red\",\n",
    "            s=6,\n",
    "        )\n",
    "        ax.set_xlabel(\"X\")\n",
    "        ax.set_ylabel(\"Y\")\n",
    "        ax.grid(False)\n",
    "    fig.suptitle(\n",
    "        f\"Initial positions of objects in {plate_name} (rescaled to {TRUE_IMAGE_SIZE}px)\",\n",
    "        y=0.9,\n",
    "    )\n",
    "    lines, labels = axs[1].get_legend_handles_labels()\n",
    "    fig.legend(lines, labels, bbox_to_anchor=(0.9, 0.95), fontsize=12)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "for idx in range(len(plate_names)):\n",
    "    plate_name = plate_names[idx]\n",
    "    experiments_path = experiments_paths[idx]\n",
    "    # get starting positions in handy format\n",
    "    this_plate_initial_true_positions_flattened_array = np.array(\n",
    "        list(initial_true_positions_arrays[plate_name].values())\n",
    "    )\n",
    "    this_plate_initial_gen_positions_flattened_array = np.array(\n",
    "        list(initial_gen_positions_arrays[plate_name].values())\n",
    "    )\n",
    "    print(f\"Plate {plate_name} - Experiment {experiments_names[idx]}\")\n",
    "    print(\n",
    "        f\"    Found {this_plate_initial_true_positions_flattened_array.shape[0]} objects in the true data\"\n",
    "    )\n",
    "    print(\n",
    "        f\"    Found {this_plate_initial_gen_positions_flattened_array.shape[0]} objects in the synthetic data\"\n",
    "    )\n",
    "    plot_starting_pos(\n",
    "        plate_name,\n",
    "        experiments_path,\n",
    "        this_plate_initial_true_positions_flattened_array,\n",
    "        this_plate_initial_gen_positions_flattened_array,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the L2 distance matrices\n",
    "distance_matrices = {}\n",
    "\n",
    "for plate_name in plate_names:\n",
    "    distance_matrices[plate_name] = cdist(\n",
    "        np.array(list(initial_true_positions_arrays[plate_name].values())),\n",
    "        np.array(list(initial_gen_positions_arrays[plate_name].values())),\n",
    "        metric=\"euclidean\",\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax = plt.gca()\n",
    "    plt.imshow(distance_matrices[plate_name], cmap=\"viridis\", origin=\"lower\")\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(cax=cax)\n",
    "    ax.set_ylabel(\"True objects\")\n",
    "    ax.set_xlabel(\"Generated objects\")\n",
    "    ax.set_title(\n",
    "        \"L2 Distance between all True and Generated Cells\\n(positions normalized by image size)\"\n",
    "    )\n",
    "    plt.suptitle(\n",
    "        f\"Plate {plate_name} - Experiment {experiments_names[0]}\",\n",
    "        y=0.9,\n",
    "    )\n",
    "    ax.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the log-log L2 distance matrix\n",
    "for plate_name in plate_names:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax = plt.gca()\n",
    "    plt.imshow(np.log(distance_matrices[plate_name]), cmap=\"viridis\", origin=\"lower\")\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(cax=cax)\n",
    "    ax.set_ylabel(\"True objects\")\n",
    "    ax.set_xlabel(\"Generated objects\")\n",
    "    ax.set_title(\n",
    "        \"L2 Distance (log-scaled) between all True and Generated Cells\\n(positions normalized by image size)\"\n",
    "    )\n",
    "    plt.suptitle(\n",
    "        f\"Plate {plate_name} - Experiment {experiments_names[0]}\",\n",
    "        y=0.9,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    ax.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plate_name in plate_names:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(distance_matrices[plate_name].min(axis=1), \"x\", label=\"Minimum distance\")\n",
    "\n",
    "    closest_objects = np.partition(distance_matrices[plate_name], [0, 1, 2], axis=1)\n",
    "    assert np.all(distance_matrices[plate_name].min(axis=1) == closest_objects[:, 0])\n",
    "\n",
    "    plt.plot(closest_objects[:, 1], \"x\", color=\"blue\", label=\"Second closest distance\")\n",
    "    plt.plot(closest_objects[:, 2], \"x\", color=\"green\", label=\"Third closest distance\")\n",
    "\n",
    "    plt.xlabel(\"True objects\")\n",
    "    plt.ylabel(\"Minimum distance to all generated objects\")\n",
    "    plt.title(\"Minimum distance to all generated objects for each true object\")\n",
    "    plt.legend()\n",
    "    plt.suptitle(f\"Plate {plate_name} - Experiment {experiments_names[0]}\")\n",
    "    plt.ylim(0, 0.07)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find closest generated cell to each true cell within threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plate_name in plate_names:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(distance_matrices[plate_name].min(axis=1), \"x\", label=\"Minimum distance\")\n",
    "\n",
    "    closest_objects = np.partition(distance_matrices[plate_name], [0, 1, 2], axis=1)\n",
    "    assert np.all(distance_matrices[plate_name].min(axis=1) == closest_objects[:, 0])\n",
    "\n",
    "    plt.plot(closest_objects[:, 1], \"x\", color=\"blue\", label=\"Second closest distance\")\n",
    "    plt.plot(closest_objects[:, 2], \"x\", color=\"green\", label=\"Third closest distance\")\n",
    "\n",
    "    plt.xlabel(\"True objects\")\n",
    "    plt.ylabel(\"Minimum distance to all generated objects\")\n",
    "    plt.title(\"Minimum distance to all generated objects for each true object\")\n",
    "    plt.legend()\n",
    "    plt.suptitle(f\"Plate {plate_name} - Experiment {experiments_names[0]}\")\n",
    "    plt.ylim(0, threshold * 2)\n",
    "    plt.axhline(y=threshold, color=\"k\", linestyle=\"--\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(plate_names)):\n",
    "    plate_name = plate_names[idx]\n",
    "    experiments_path = experiments_paths[idx]\n",
    "    # get starting positions in handy format\n",
    "    this_plate_initial_true_positions_flattened_array = np.array(\n",
    "        list(initial_true_positions_arrays[plate_name].values())\n",
    "    )\n",
    "    this_plate_initial_gen_positions_flattened_array = np.array(\n",
    "        list(initial_gen_positions_arrays[plate_name].values())\n",
    "    )\n",
    "    print(f\"Plate {plate_name} - Experiment {experiments_names[idx]}\")\n",
    "    print(\n",
    "        f\"    Found {this_plate_initial_true_positions_flattened_array.shape[0]} objects in the true data\"\n",
    "    )\n",
    "    print(\n",
    "        f\"    Found {this_plate_initial_gen_positions_flattened_array.shape[0]} objects in the synthetic data\"\n",
    "    )\n",
    "    plot_starting_pos(\n",
    "        plate_name,\n",
    "        experiments_path,\n",
    "        this_plate_initial_true_positions_flattened_array,\n",
    "        this_plate_initial_gen_positions_flattened_array,\n",
    "        circles=True,\n",
    "        threshold=threshold,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_cells_to_gen_cells_mapping = {}\n",
    "\n",
    "for obj_id in tqdm(these_plates_true_features_time_1[\"global_object_id\"].unique()):\n",
    "    plate_name = obj_id.split(\"-\")[0]\n",
    "    assert plate_name in plate_names, (\n",
    "        f\"Plate name {plate_name} not found in plate names {plate_names}\"\n",
    "    )\n",
    "\n",
    "    obj_index = list(initial_true_positions_arrays[plate_name]).index(obj_id)\n",
    "\n",
    "    distances_to_this_obj = distance_matrices[plate_name][obj_index]\n",
    "    indices_below_threshold = np.nonzero(distances_to_this_obj < threshold)[0]\n",
    "\n",
    "    if len(indices_below_threshold) > 0:\n",
    "        # Find the index with minimum distance among those below threshold\n",
    "        min_distance_idx = indices_below_threshold[\n",
    "            np.argmin(distances_to_this_obj[indices_below_threshold])\n",
    "        ]\n",
    "\n",
    "        # Get the corresponding generated cell ID\n",
    "        closest_gen_id = list(initial_gen_positions_arrays[plate_name])[min_distance_idx]\n",
    "\n",
    "        true_cells_to_gen_cells_mapping[obj_id] = {\n",
    "            \"closest_gen_id\": closest_gen_id,\n",
    "            \"min_distance\": float(distances_to_this_obj[min_distance_idx]),\n",
    "            \"base_true_position\": tuple(initial_true_positions_object_ids_to_pos[obj_id]),\n",
    "        }\n",
    "    else:\n",
    "        # No matches below threshold\n",
    "        true_cells_to_gen_cells_mapping[obj_id] = {\n",
    "            \"closest_gen_id\": None,\n",
    "            \"min_distance\": None,\n",
    "            \"base_true_position\": tuple(initial_true_positions_object_ids_to_pos[obj_id]),\n",
    "        }\n",
    "\n",
    "no_matches = list(\n",
    "    filter(\n",
    "        lambda p: p[1][\"closest_gen_id\"] is None,\n",
    "        true_cells_to_gen_cells_mapping.items(),\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    f\"{len(no_matches)} out of {len(true_cells_to_gen_cells_mapping)} true cells have no matches in the generated data ({len(no_matches) / len(true_cells_to_gen_cells_mapping) * 100:.1f}%)\"\n",
    ")\n",
    "print(\"Mapping:\")\n",
    "display(true_cells_to_gen_cells_mapping)\n",
    "\n",
    "for true_id, mapping in true_cells_to_gen_cells_mapping.items():\n",
    "    if mapping[\"closest_gen_id\"] is not None:\n",
    "        plate_name = true_id.split(\"-\")[0]\n",
    "        gen_id = mapping[\"closest_gen_id\"]\n",
    "        min_dist = mapping[\"min_distance\"]\n",
    "\n",
    "        assert plate_name == gen_id.split(\"-\")[0], (\n",
    "            f\"Plate names do not match: {true_id} vs {gen_id}\"\n",
    "        )\n",
    "        assert mapping[\"base_true_position\"] == tuple(\n",
    "            initial_true_positions_object_ids_to_pos[true_id]\n",
    "        ), (\n",
    "            f\"True position does not match: {mapping['base_true_position']} vs {initial_true_positions_object_ids_to_pos[true_id]}\"\n",
    "        )\n",
    "        assert min_dist == np.min(\n",
    "            distance_matrices[plate_name][\n",
    "                list(initial_true_positions_arrays[plate_name]).index(true_id)\n",
    "            ]\n",
    "        ), (\n",
    "            f\"Minimum distance does not match: {min_dist} vs {np.min(distance_matrices[plate_name][list(initial_true_positions_arrays[plate_name]).index(true_id)])}\"\n",
    "        )\n",
    "        assert min_dist < threshold, (\n",
    "            f\"Minimum distance {min_dist} is not below threshold {threshold}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process not-simple objects\n",
    "\n",
    "That is: (true and/or generated) objects that merge, or split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter pairs on full lifetime (true and) generated objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get expected full lifetime per generation experiment (= per plate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_gen_frames_per_field = (\n",
    "    gen_features.groupby(\"plate_name\")[\"TrackObjects_FinalAge_10\"].max().astype(int)\n",
    ")\n",
    "nb_gen_frames_per_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_TRUE_FRAMES = 19\n",
    "NB_GEN_FRAMES = nb_gen_frames_per_field.to_dict()\n",
    "\n",
    "print(\"NB_TRUE_FRAMES:\", NB_TRUE_FRAMES)\n",
    "print(\"NB_GEN_FRAMES:\", NB_GEN_FRAMES)\n",
    "assert set(NB_GEN_FRAMES.keys()) == set(plate_names), (\n",
    "    f\"Plate names in NB_GEN_FRAMES do not match the expected plate names: {plate_names}. Found: {NB_GEN_FRAMES.keys()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_true_ids = []\n",
    "\n",
    "# these reasons are mutually exclusive and ordered!\n",
    "reasons_skipped_count = {\n",
    "    \"no_matching_gen_cell\": 0,\n",
    "    \"gen_no_full_lifetime\": 0,\n",
    "    \"not_simple_true_object\": 0,\n",
    "    \"not_simple_gen_object\": 0,\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"Getting object types and skipping true objects in {plate_names}:\")\n",
    "for true_object_id in tqdm(true_cells_to_gen_cells_mapping):\n",
    "    plate_name = true_object_id.split(\"-\")[0]\n",
    "\n",
    "    this_obj_true_features = these_plates_true_features.query(\"global_object_id == @true_object_id\")\n",
    "    matching_gen_id = true_cells_to_gen_cells_mapping[true_object_id][\"closest_gen_id\"]\n",
    "    matching_obj_gen_features = gen_features.query(\"global_object_id == @matching_gen_id\")\n",
    "\n",
    "    true_cells_to_gen_cells_mapping[true_object_id][\"object_type\"] = []\n",
    "\n",
    "    # 1. No initial match\n",
    "    if matching_gen_id is None:\n",
    "        print(f\"{true_object_id} with no matching generated cell\")\n",
    "        reasons_skipped_count[\"no_matching_gen_cell\"] += 1\n",
    "        true_cells_to_gen_cells_mapping[true_object_id][\"object_type\"].append(\n",
    "            \"no_matching_gen_cell\"\n",
    "        )\n",
    "        continue  # skip this true object\n",
    "\n",
    "    # 2. Not simple true object\n",
    "    # this filtering should have been performed before\n",
    "    assert len(this_obj_true_features) >= NB_TRUE_FRAMES, (\n",
    "        f\"Expected only full lifetime true cells, but found {len(this_obj_true_features)} timepoints for {true_object_id}\"\n",
    "    )\n",
    "    if len(this_obj_true_features) != NB_TRUE_FRAMES:  # so >\n",
    "        true_cells_to_gen_cells_mapping[true_object_id][\"object_type\"].append(\n",
    "            \"not_simple_true_object\"\n",
    "        )\n",
    "\n",
    "    # 3. Not simple generated object\n",
    "    if len(matching_obj_gen_features) > NB_GEN_FRAMES[plate_name]:\n",
    "        true_cells_to_gen_cells_mapping[true_object_id][\"object_type\"].append(\n",
    "            \"not_simple_gen_object\"\n",
    "        )\n",
    "\n",
    "    # 4. Not full lifetime generated object\n",
    "    gen_final_age = matching_obj_gen_features[\"TrackObjects_FinalAge_10\"].dropna().unique()\n",
    "    assert len(gen_final_age) == 1, (\n",
    "        f\"Expected only one final age, but found {len(gen_final_age)} for true id {true_object_id} & matching gen id {matching_gen_id}\"\n",
    "    )\n",
    "    gen_final_age = gen_final_age[0]\n",
    "    if gen_final_age != NB_GEN_FRAMES[plate_name]:\n",
    "        print(\n",
    "            f\"{true_object_id} with no full lifetime of matching generated cell having {int(gen_final_age)} < {NB_GEN_FRAMES[plate_name]} final age\"\n",
    "        )\n",
    "        reasons_skipped_count[\"gen_no_full_lifetime\"] += 1\n",
    "        true_cells_to_gen_cells_mapping[true_object_id][\"object_type\"].append(\n",
    "            \"gen_no_full_lifetime\"\n",
    "        )\n",
    "        continue  # skip this true object\n",
    "\n",
    "    kept_true_ids.append(true_object_id)\n",
    "\n",
    "    if true_cells_to_gen_cells_mapping[true_object_id][\"object_type\"] == []:\n",
    "        true_cells_to_gen_cells_mapping[true_object_id][\"object_type\"].append(\"simple_true_&_gen\")\n",
    "\n",
    "orig_kept_true_ids = kept_true_ids.copy()\n",
    "print(f\"\\nKept {len(kept_true_ids)} true IDs: {kept_true_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Reasons of filtering (ordered & mutually exclusive, some reasons actually do not filter anymore):\"\n",
    ")\n",
    "for k, v in reasons_skipped_count.items():\n",
    "    print(f\"    {k}: {v} ({v / len(true_cells_to_gen_cells_mapping) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check nb cells kept per plate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Kept {len(kept_true_ids)} out of {len(true_cells_to_gen_cells_mapping)} true cells ({len(kept_true_ids) / len(true_cells_to_gen_cells_mapping) * 100:.1f}%) from {len(plate_names)} plates\"\n",
    ")\n",
    "print(\"Number of true cells per plate:\")\n",
    "for plate_name in plate_names:\n",
    "    print(\n",
    "        f\"    {plate_name}: {len([k for k in kept_true_ids if k.startswith(plate_name)])} out of {len([k for k in true_cells_to_gen_cells_mapping if k.startswith(plate_name)])} ({len([k for k in kept_true_ids if k.startswith(plate_name)]) / len([k for k in true_cells_to_gen_cells_mapping if k.startswith(plate_name)]) * 100:.1f}%)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check object types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_type_counts = {}\n",
    "for true_obj_id, data in true_cells_to_gen_cells_mapping.items():\n",
    "    for types in data[\"object_type\"]:\n",
    "        if types not in object_type_counts:\n",
    "            object_type_counts[types] = 0\n",
    "        object_type_counts[types] += 1\n",
    "\n",
    "print(\"Type of objects (unordered & not exclusive!):\")\n",
    "for k, v in object_type_counts.items():\n",
    "    if k in (\"simple_true_&_gen\", \"not_simple_true_object\", \"not_simple_gen_object\"):\n",
    "        print(\n",
    "            f\"    {k}: {v} ({v / len(true_cells_to_gen_cells_mapping) * 100:.1f}% of total) ({v / len(kept_true_ids) * 100:.1f}% of kept)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"    {k}: {v} ({v / len(true_cells_to_gen_cells_mapping) * 100:.1f}% of total)\")\n",
    "\n",
    "\n",
    "assert (\n",
    "    len(kept_true_ids)\n",
    "    == len(true_cells_to_gen_cells_mapping)\n",
    "    - reasons_skipped_count[\"no_matching_gen_cell\"]\n",
    "    - reasons_skipped_count[\"gen_no_full_lifetime\"]\n",
    "), (\n",
    "    f\"Something went wrong: {len(kept_true_ids)} != {len(true_cells_to_gen_cells_mapping)} - {reasons_skipped_count['no_matching_gen_cell']} - {reasons_skipped_count['gen_no_full_lifetime']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Duplicate\" non-simple objects by reconstructing their full trajectory with another ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tracking_tree(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"\n",
    "    Builds a directed graph representation of the tracking data showing parent-child relationships.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing trajectory data\n",
    "\n",
    "    Returns:\n",
    "        tuple: (DiGraph, positions_dict) where:\n",
    "            - DiGraph is the networkx graph of the tracking tree\n",
    "            - positions_dict contains node positioning information for visualization\n",
    "    \"\"\"\n",
    "    # Sort by time and create directed graph\n",
    "    time_key = \"Metadata_time\" if \"Metadata_time\" in df.columns else \"time\"\n",
    "    df_sorted = df.sort_values(time_key)\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes and edges in one pass\n",
    "    nodes_by_time = {}\n",
    "    for _, row in df_sorted.iterrows():\n",
    "        time = int(row[time_key])\n",
    "        obj_num = int(row[\"ObjectNumber\"])  # per-time id\n",
    "        obj_id = row[\"global_object_id\"]  # per-object id\n",
    "        track_label = row[\"TrackObjects_Label_10\"]\n",
    "\n",
    "        # Node attributes and grouping by time\n",
    "        # we use object number and not tracking label as node id\n",
    "        # as the link between objects at t and t+1 is done on object number by CP\n",
    "        node_id = (time, obj_num, obj_id)\n",
    "        label = f\"{obj_num}\\n({track_label})\"\n",
    "        G.add_node(node_id, label=label, time=time)\n",
    "\n",
    "        # Group nodes by time for positioning\n",
    "        if time not in nodes_by_time:\n",
    "            nodes_by_time[time] = []\n",
    "        nodes_by_time[time].append(node_id)\n",
    "\n",
    "        # Add edge if parent exists\n",
    "        parent_num = row[\"TrackObjects_ParentObjectNumber_10\"]\n",
    "        if pd.notna(parent_num) and parent_num > 0:\n",
    "            parent_id = (time - 1, parent_num, obj_id)\n",
    "            if parent_id in G.nodes:\n",
    "                G.add_edge(parent_id, node_id)\n",
    "\n",
    "    # Create positions for nodes\n",
    "    positions = {}\n",
    "    for time, nodes in nodes_by_time.items():\n",
    "        num_nodes = len(nodes)\n",
    "        for i, node_id in enumerate(sorted(nodes, key=lambda x: x[1])):\n",
    "            positions[node_id] = (time, i - (num_nodes - 1) / 2)\n",
    "\n",
    "    return G, positions\n",
    "\n",
    "\n",
    "def get_all_tree_paths(G: nx.DiGraph, progress: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Extracts all paths from root nodes to leaf nodes in the tracking tree.\n",
    "\n",
    "    Args:\n",
    "        G: NetworkX DiGraph object representing the tracking tree\n",
    "        progress: Whether to show a progress bar\n",
    "\n",
    "    Returns:\n",
    "        list: List of paths, where each path is a list of node IDs from root to leaf\n",
    "    \"\"\"\n",
    "    # Find root nodes (those with no predecessors)\n",
    "    root_nodes = [node for node in G.nodes() if G.in_degree(node) == 0]\n",
    "\n",
    "    all_paths = []\n",
    "\n",
    "    # Set up progress bar\n",
    "    if progress:\n",
    "        pbar = tqdm(total=len(root_nodes), desc=\"Processing root nodes\")\n",
    "\n",
    "    # Process each root node with iterative DFS\n",
    "    for root in root_nodes:\n",
    "        # Use a stack for iterative DFS\n",
    "        stack = [(root, [root])]\n",
    "\n",
    "        while stack:\n",
    "            node, path = stack.pop()\n",
    "\n",
    "            # If this is a leaf node, we've found a complete path\n",
    "            if G.out_degree(node) == 0:\n",
    "                all_paths.append(path)\n",
    "            else:\n",
    "                # Add all child nodes to the stack with their paths\n",
    "                for successor in G.successors(node):\n",
    "                    stack.append((successor, path + [successor]))\n",
    "\n",
    "        if progress:\n",
    "            pbar.update(1)\n",
    "\n",
    "    if progress:\n",
    "        pbar.close()\n",
    "\n",
    "    return all_paths\n",
    "\n",
    "\n",
    "def plot_tracking_tree(G: nx.DiGraph, positions: dict, ax=None, figsize: tuple = (8, 6)):\n",
    "    \"\"\"\n",
    "    Plots a visual tree representation of the tracking data showing parent-child relationships.\n",
    "\n",
    "    Args:\n",
    "        G: NetworkX DiGraph object representing the tracking tree\n",
    "        positions: Dictionary mapping node IDs to (x,y) positions\n",
    "        ax: Optional matplotlib axis to plot on\n",
    "        figsize: Figure size as (width, height) in inches\n",
    "    \"\"\"\n",
    "    # Create plot\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Draw the graph (edges, nodes, labels)\n",
    "    nx.draw_networkx_edges(G, positions, ax=ax, arrows=True, arrowstyle=\"->\", width=1.2)\n",
    "    nx.draw_networkx_nodes(G, positions, ax=ax, node_color=\"lightblue\")\n",
    "    nx.draw_networkx_labels(\n",
    "        G,\n",
    "        positions,\n",
    "        labels={n: G.nodes[n][\"label\"] for n in G.nodes()},\n",
    "        font_size=5,\n",
    "        font_color=\"black\",\n",
    "    )\n",
    "\n",
    "    # Get the time range from the graph\n",
    "    time_points = sorted(set(G.nodes[n][\"time\"] for n in G.nodes()))\n",
    "\n",
    "    # Customize plot appearance\n",
    "    if time_points:\n",
    "        time_min, time_max = min(time_points), max(time_points)\n",
    "        ax.set_title(f\"Tracking Tree (Time {time_min} to {time_max})\")\n",
    "    else:\n",
    "        ax.set_title(\"Tracking Tree\")\n",
    "\n",
    "    ax.set_xlabel(\"Time\")\n",
    "\n",
    "    # Remove y-axis ticks as they don't represent anything specific\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Reset x-axis formatting completely\n",
    "    ax.tick_params(reset=True)\n",
    "\n",
    "    # Force x-axis visibility\n",
    "    ax.spines[\"bottom\"].set_visible(True)\n",
    "    ax.spines[\"bottom\"].set_linewidth(1.0)\n",
    "    ax.set_xticks(time_points)\n",
    "    ax.set_xticklabels(time_points)\n",
    "    ax.tick_params(\n",
    "        axis=\"x\",\n",
    "        which=\"both\",\n",
    "        length=4,\n",
    "        width=1,\n",
    "        colors=\"black\",\n",
    "        labelcolor=\"black\",\n",
    "        bottom=True,\n",
    "        labelbottom=True,\n",
    "    )\n",
    "    ax.xaxis.set_tick_params(which=\"both\", bottom=True, labelbottom=True)\n",
    "\n",
    "    # Add a light grid\n",
    "    ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Make plot more compact\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize few non-simple examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ids_non_simple_true_trajs = [\n",
    "    k\n",
    "    for k, v in true_cells_to_gen_cells_mapping.items()\n",
    "    if \"not_simple_true_object\" in v[\"object_type\"]\n",
    "]\n",
    "\n",
    "for id in random.sample(true_ids_non_simple_true_trajs, 3) + [\"A_13_fld_2-14.0\"]:\n",
    "    print(\"True ID\", id)\n",
    "    tmp_df = these_plates_true_features.query(\"global_object_id == @id\")\n",
    "\n",
    "    # Build the tracking tree\n",
    "    G, positions = build_tracking_tree(tmp_df)\n",
    "\n",
    "    # Find all paths from roots to leaves\n",
    "    all_paths = get_all_tree_paths(G)\n",
    "\n",
    "    # Print path information\n",
    "    print(\"\\nPaths through tracking tree:\")\n",
    "    for i, path in enumerate(all_paths):\n",
    "        path_str = \" → \".join([str(o) for t, o, l in path])\n",
    "        print(f\"Path {i + 1}: {path_str}\")\n",
    "\n",
    "    # Plot tree visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 5), dpi=150)\n",
    "    plot_tracking_tree(G, positions, ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "    assert len(tmp_df) == len(G), (\n",
    "        f\"Expected the same number of nodes in the graph than rows in the DataFrame, got {len(tmp_df)} rows and {len(G)} nodes\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the full tree of non-simple true objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the full tracking tree\n",
    "G, positions = build_tracking_tree(\n",
    "    these_plates_true_features.query(\"global_object_id in @true_ids_non_simple_true_trajs\")\n",
    ")\n",
    "\n",
    "# Find all paths from roots to leaves\n",
    "all_paths = get_all_tree_paths(G)\n",
    "\n",
    "# Print path information\n",
    "print(\n",
    "    f\"Paths through tracking tree of {len(true_ids_non_simple_true_trajs)} non-simple true objects:\"\n",
    ")\n",
    "for i, path in enumerate(all_paths):\n",
    "    path_str = \" → \".join([str(o) for _, o, l in path])\n",
    "    print(f\"Path {i + 1}: {path_str}\")\n",
    "\n",
    "# # Plot tree visualization  (commented because matplotlib is SOOO SLOOOW)\n",
    "# fig, ax = plt.subplots(figsize=(30, 60), dpi=150)\n",
    "# plot_tracking_tree(G, positions, ax=ax)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No mergers because of the CP pipeline keeping track of only *the* closest parent cell...... ... ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_nodes = [node for node in G.nodes() if G.in_degree(node) == 0]\n",
    "leaf_nodes = [node for node in G.nodes() if G.out_degree(node) == 0]\n",
    "print(f\"Found {len(root_nodes)} root nodes and {len(leaf_nodes)} leaf nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_splitting_trajs(\n",
    "    obj_id: str, this_obj_features: pd.DataFrame, features: pd.DataFrame, full_lifetime: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns the `features` dataframe with subtrajs with non full lifetime removed and duplicated subtrajs with full lifetime,\n",
    "    using the graph computed from `this_obj_features`.\n",
    "\n",
    "    Beware: this function does not check if `features` was already processed, (will result in duplicated rows),\n",
    "    and is far from optimized for speed.\n",
    "    \"\"\"\n",
    "    # Build the tracking tree for this object\n",
    "    G, _ = build_tracking_tree(this_obj_features)\n",
    "    # Find all paths from roots to leaves\n",
    "    all_paths: list[list[tuple[int, int]]] = get_all_tree_paths(G)\n",
    "    nodes_to_keep: set[tuple[int, int]] = set()\n",
    "    for path in all_paths:\n",
    "        if len(path) == full_lifetime:\n",
    "            # duplicate the full lifetime subpath to be a full path, in-place in features\n",
    "            features = add_path_to_df(path, features, obj_id)\n",
    "            # add the nodes from that full lifetime path to the ones to be kept\n",
    "            nodes_to_keep = nodes_to_keep.union(set(path))\n",
    "\n",
    "    # finally remove the non full lifetime paths not part of the full lifetime paths\n",
    "    # (ie: prune the tree!)\n",
    "    nodes_to_prune = set(G.nodes()) - nodes_to_keep\n",
    "    if len(nodes_to_prune) != 0:\n",
    "        remove_nodes_from_df(nodes_to_prune, features, obj_id)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def remove_nodes_from_df(nodes: set, features: pd.DataFrame, obj_id: str):\n",
    "    \"\"\"\n",
    "    Removes `nodes` of the trajectories of `obj_id` in the `features` DataFrame\n",
    "\n",
    "    Beware: this function modifies the DataFrame in place.\n",
    "    \"\"\"\n",
    "    this_obj_maks = features[\"global_object_id\"] == obj_id\n",
    "    time_key = \"Metadata_time\" if \"Metadata_time\" in features.columns else \"time\"\n",
    "    indexes_to_remove = []\n",
    "    for t, o, l in nodes:\n",
    "        # get this object at this time\n",
    "        this_time_this_obj_mask = (features[time_key] == t) & (this_obj_maks)\n",
    "        this_time_this_obj = features[this_time_this_obj_mask]\n",
    "        # checks\n",
    "        if len(this_time_this_obj) <= 1:\n",
    "            # if we remove something at this time, another one should exist\n",
    "            raise ValueError(\n",
    "                f\"Expected at least 2 object numbers at time {t} for global object {obj_id}, but found none\"\n",
    "            )\n",
    "        else:\n",
    "            # now get this subpath for this global object at this time\n",
    "            this_subtraj_mask = this_time_this_obj[\"ObjectNumber\"] == o\n",
    "            this_time_this_obj_this_subtraj = this_time_this_obj[this_subtraj_mask]\n",
    "            assert len(this_time_this_obj_this_subtraj) == 1, (\n",
    "                f\"Expected one subobject at time {t} for obj_id={obj_id} and subobject {o}, but found {len(this_time_this_obj_this_subtraj)}\"\n",
    "            )\n",
    "            index_to_rm = this_time_this_obj_this_subtraj.index[0]\n",
    "            indexes_to_remove.append(index_to_rm)\n",
    "    assert len(indexes_to_remove) > 0, (\n",
    "        f\"Expected to remove at least one timepoint for {obj_id}, but found none\"\n",
    "    )\n",
    "    features.drop(index=indexes_to_remove, inplace=True)\n",
    "    print(f\"  Removed {len(indexes_to_remove)} timepoints from {obj_id}\")\n",
    "\n",
    "\n",
    "def add_path_to_df(path: list, features: pd.DataFrame, obj_id: str):\n",
    "    \"\"\"\n",
    "    Duplicates a subpath of a trajectory in the DataFrame with a new global object id\n",
    "\n",
    "    Beware: this function returns a reindexed dataframe\n",
    "    \"\"\"\n",
    "    this_obj_maks = features[\"global_object_id\"] == obj_id\n",
    "    time_key = \"Metadata_time\" if \"Metadata_time\" in features.columns else \"time\"\n",
    "    indexes_to_duplicate = []\n",
    "    for t, o, l in path:\n",
    "        this_time_this_obj_mask = (features[time_key] == t) & (this_obj_maks)\n",
    "        this_time_this_obj = features[this_time_this_obj_mask]\n",
    "        if len(this_time_this_obj) == 0:\n",
    "            raise ValueError(\n",
    "                f\"Expected at least one object at time {t} for {obj_id}, but found none\"\n",
    "            )\n",
    "        else:\n",
    "            this_time_this_obj_this_subtraj_mask = this_time_this_obj[\"ObjectNumber\"] == o\n",
    "            this_time_this_obj_this_subtraj = this_time_this_obj[\n",
    "                this_time_this_obj_this_subtraj_mask\n",
    "            ]\n",
    "            assert len(this_time_this_obj_this_subtraj) == 1, (\n",
    "                f\"Expected one subobject at time {t} for obj_id={obj_id} and subobject {o}, but found {len(this_time_this_obj_this_subtraj)}\"\n",
    "            )\n",
    "            index_to_duplicate = this_time_this_obj_this_subtraj.index[0]\n",
    "            indexes_to_duplicate.append(index_to_duplicate)\n",
    "    assert len(np.unique(indexes_to_duplicate)) == len(indexes_to_duplicate) == len(path)\n",
    "    # now add these rows to the dataframe with a new global object id\n",
    "    subtraj_nb = 0\n",
    "    new_obj_id = f\"{obj_id}-extracted_subtraj_{subtraj_nb}\"\n",
    "    while new_obj_id in features[\"global_object_id\"].values:\n",
    "        subtraj_nb += 1\n",
    "        new_obj_id = f\"{obj_id}-extracted_subtraj_{subtraj_nb}\"\n",
    "    features_to_add = features.loc[indexes_to_duplicate].copy(deep=True)\n",
    "    features_to_add[\"global_object_id\"] = new_obj_id\n",
    "    features = pd.concat([features, features_to_add], ignore_index=True)\n",
    "    print(\n",
    "        f\"  Added {len(features_to_add)} timepoints to {new_obj_id} ({len(path)} timepoints in the path)\"\n",
    "    )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform the processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some checks to ensure no double-processing\n",
    "assert not these_plates_true_features[\"global_object_id\"].str.contains(\"-extracted_subtraj_\").any()\n",
    "assert not gen_features[\"global_object_id\"].str.contains(\"-extracted_subtraj_\").any()\n",
    "\n",
    "for true_obj_id, data in tqdm(true_cells_to_gen_cells_mapping.items()):\n",
    "    if data[\"object_type\"] == [\"simple_true_&_gen\"]:\n",
    "        continue\n",
    "    elif data[\"object_type\"] == [\"no_matching_gen_cell\"]:\n",
    "        continue\n",
    "    elif \"gen_no_full_lifetime\" in data[\"object_type\"]:\n",
    "        continue\n",
    "    else:\n",
    "        plate_name = true_obj_id.split(\"-\")[0]\n",
    "        this_obj_true_features = these_plates_true_features.query(\n",
    "            \"global_object_id == @true_obj_id\"\n",
    "        )\n",
    "        matching_gen_id = true_cells_to_gen_cells_mapping[true_obj_id][\"closest_gen_id\"]\n",
    "        matching_obj_gen_features = gen_features.query(\"global_object_id == @matching_gen_id\")\n",
    "\n",
    "        object_type_ok = False\n",
    "\n",
    "        if \"not_simple_true_object\" in data[\"object_type\"]:\n",
    "            object_type_ok = True\n",
    "            print(f\"Processing true {true_obj_id}\")\n",
    "            these_plates_true_features = process_splitting_trajs(\n",
    "                true_obj_id, this_obj_true_features, these_plates_true_features, NB_TRUE_FRAMES\n",
    "            )\n",
    "            # remove the original non-simple objects\n",
    "            these_plates_true_features.drop(\n",
    "                these_plates_true_features.query(\"global_object_id == @true_obj_id\").index,\n",
    "                inplace=True,\n",
    "            )\n",
    "\n",
    "        if \"not_simple_gen_object\" in data[\"object_type\"]:\n",
    "            object_type_ok = True\n",
    "            print(f\"Processing gen {matching_gen_id}\")\n",
    "            gen_features = process_splitting_trajs(\n",
    "                matching_gen_id, matching_obj_gen_features, gen_features, NB_GEN_FRAMES[plate_name]\n",
    "            )\n",
    "            # remove the original non-simple objects\n",
    "            gen_features.drop(\n",
    "                gen_features.query(\"global_object_id == @matching_gen_id\").index,\n",
    "                inplace=True,\n",
    "            )\n",
    "\n",
    "        if not object_type_ok:\n",
    "            raise ValueError(f\"Unknown object type: {data['object_type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = these_plates_true_features.loc[\n",
    "    these_plates_true_features[\"global_object_id\"].str.startswith(\"M_13_fld_3-76.0\")\n",
    "]\n",
    "\n",
    "print(f\"Shown gloabl_object_id: {tmp_df['global_object_id'].unique()}\")\n",
    "\n",
    "G, positions = build_tracking_tree(tmp_df)\n",
    "\n",
    "# Find all paths from roots to leaves\n",
    "all_paths = get_all_tree_paths(G)\n",
    "\n",
    "# Print path information\n",
    "print(\"\\nPaths through tracking tree:\")\n",
    "for i, path in enumerate(all_paths):\n",
    "    path_str = \" → \".join([str(o) for t, o, l in path])\n",
    "    print(f\"Path {i + 1}: {path_str}\")\n",
    "\n",
    "# Plot tree visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 5), dpi=150)\n",
    "plot_tracking_tree(G, positions, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "these_plates_true_features.loc[\n",
    "    these_plates_true_features[\"global_object_id\"].str.startswith(\"M_13_fld_3-76.0\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "these_plates_true_features[\"global_object_id\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update `kept_true_ids` wih the new IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_type_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in tqdm(orig_kept_true_ids):\n",
    "    assert \"-extracted_subtraj_\" not in id, (\n",
    "        f\"Expected {id} to be a simple object, but found -extracted_subtraj_ in the ID\"\n",
    "    )\n",
    "    if id not in these_plates_true_features[\"global_object_id\"].unique():\n",
    "        # check the now missing IDs are only non-simple objects\n",
    "        obj_type = true_cells_to_gen_cells_mapping[id][\"object_type\"]\n",
    "        assert \"not_simple_true_object\" in obj_type or \"not_simple_gen_object\" in obj_type, (\n",
    "            f\"Expected {id} to be a non-simple object, but found {obj_type}\"\n",
    "        )\n",
    "        # add the newly created IDs to the kept IDs\n",
    "        new_ids = these_plates_true_features.query(\"global_object_id.str.startswith(@id)\")[\n",
    "            \"global_object_id\"\n",
    "        ].unique()\n",
    "        kept_true_ids.extend(new_ids)\n",
    "        # remove the original ID from the kept IDs\n",
    "        kept_true_ids.remove(id)\n",
    "\n",
    "print(f\"Now having {len(kept_true_ids)} true cell IDs (vs {len(orig_kept_true_ids)} before)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More checks after data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "G, positions = build_tracking_tree(\n",
    "    these_plates_true_features.query(\"global_object_id in @kept_true_ids\")\n",
    ")\n",
    "\n",
    "# Find all paths from roots to leaves\n",
    "all_paths = get_all_tree_paths(G)\n",
    "\n",
    "# Print path information\n",
    "print(\"\\nPaths through tracking tree:\")\n",
    "for i, path in enumerate(all_paths):\n",
    "    path_str = \" → \".join([str(o) for t, o, l in path])\n",
    "    print(f\"Path {i + 1}: {path_str}\")\n",
    "\n",
    "# Plot tree visualization (commented because matplotlib is SOOO SLOOOW)\n",
    "# fig, ax = plt.subplots(figsize=(30, 60), dpi=150)\n",
    "# plot_tracking_tree(G, positions, ax=ax)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find root nodes (those with no predecessors)\n",
    "root_nodes = [node for node in G.nodes() if G.in_degree(node) == 0]\n",
    "\n",
    "# Find leaf nodes (those with no successors)\n",
    "leaf_nodes = [node for node in G.nodes() if G.out_degree(node) == 0]\n",
    "\n",
    "print(len(root_nodes), \"root nodes\")\n",
    "print(len(leaf_nodes), \"leaf nodes\")\n",
    "assert len(root_nodes) == len(leaf_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show feature and L2/cosine/... distance for one cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature = \"AreaShape_Area\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_object_id = kept_true_ids[np.random.randint(0, len(kept_true_ids))]\n",
    "true_object_id = plate_name + \"-373.0\"\n",
    "true_object_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    true_object_id in kept_true_ids\n",
    "    and true_object_id in these_plates_true_features[\"global_object_id\"].values\n",
    ")\n",
    "\n",
    "this_obj_true_features = these_plates_true_features.query(\"global_object_id == @true_object_id\")\n",
    "this_obj_true_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_gen_id = true_cells_to_gen_cells_mapping[\n",
    "    re.sub(r\"-extracted_subtraj_\\d+\", \"\", true_object_id)\n",
    "][\"closest_gen_id\"]  # might not exist! (because the matching gen was itself a subtraj)\n",
    "matching_gen_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_obj_gen_features = gen_features.query(\"global_object_id == @matching_gen_id\")\n",
    "matching_obj_gen_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot feature and compute L2 /cosine sim distance of timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trajectory_comparison_metrics(\n",
    "    true_traj: Union[np.ndarray, ArrayLike], gen_traj: Union[np.ndarray, ArrayLike]\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute metrics between trajectories with different numbers of points (by interpolating the generated trajectory at the true times).\n",
    "\n",
    "    Args:\n",
    "        true_traj: array of shape (n_true_points,)\n",
    "        gen_traj: array of shape (n_gen_points,)\n",
    "\n",
    "    Returns:\n",
    "        gen_traj_interp: array of shape (n_true_points,) of the interpolated generated trajectory at the true times\n",
    "        l2: float, L2 distance between the trajectories\n",
    "        cos_sim: float, cosine similarity between the trajectories\n",
    "        normd_l2: float, L2 distance between the trajectories normalized by the true trajectory\n",
    "    \"\"\"\n",
    "    # Checks\n",
    "    assert true_traj.ndim == 1, \"true_traj should be a 1D array\"\n",
    "    assert gen_traj.ndim == 1, \"gen_traj should be a 1D array\"\n",
    "    assert len(true_traj) > 0, \"true_traj should not be empty\"\n",
    "    assert len(gen_traj) > 0, \"gen_traj should not be empty\"\n",
    "\n",
    "    # Create synthetic time values (assuming uniform sampling)\n",
    "    if len(true_traj) != len(gen_traj):\n",
    "        normalized_true_times = np.linspace(0, 1, len(true_traj))\n",
    "        normalized_gen_times = np.linspace(0, 1, len(gen_traj))\n",
    "\n",
    "        # Interpolate the generated trajectory at the true times\n",
    "        gen_traj_interp = np.interp(normalized_true_times, normalized_gen_times, gen_traj)\n",
    "    else:\n",
    "        gen_traj_interp = gen_traj\n",
    "\n",
    "    # l2_dist\n",
    "    l2 = np.linalg.norm(true_traj - gen_traj_interp)\n",
    "\n",
    "    # cosine_sim\n",
    "    cos_sim = np.dot(true_traj, gen_traj_interp) / (\n",
    "        np.linalg.norm(true_traj) * np.linalg.norm(gen_traj_interp)\n",
    "    )\n",
    "\n",
    "    # normd_l2\n",
    "    normd_l2 = l2 / np.linalg.norm(true_traj)\n",
    "\n",
    "    return gen_traj_interp, l2, cos_sim, normd_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_traj_interp, l2_dist, cosine_sim, normd_l2 = compute_trajectory_comparison_metrics(\n",
    "    this_obj_true_features[selected_feature].values,\n",
    "    matching_obj_gen_features[selected_feature].values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "# true\n",
    "plt.plot(\n",
    "    this_obj_true_features[\"Metadata_time\"],\n",
    "    this_obj_true_features[selected_feature].values,\n",
    "    \"x-\",\n",
    "    label=\"True\",\n",
    ")\n",
    "# generated\n",
    "normalized_gen_times = (\n",
    "    matching_obj_gen_features[\"time\"] / matching_obj_gen_features[\"time\"].max() * 18 + 1\n",
    ")\n",
    "plt.plot(\n",
    "    normalized_gen_times,\n",
    "    matching_obj_gen_features[selected_feature].values,\n",
    "    \"x-\",\n",
    "    label=\"Generated\",\n",
    ")\n",
    "# interpolated generated\n",
    "plt.plot(\n",
    "    this_obj_true_features[\"Metadata_time\"],\n",
    "    gen_traj_interp,\n",
    "    \"x-\",\n",
    "    label=\"Interpolated Generated\",\n",
    ")\n",
    "\n",
    "plt.title(f\"{selected_feature} for true object {true_object_id}\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(selected_feature)\n",
    "plt.xticks(np.arange(1, 20, 1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert cosine_sim == np.dot(this_obj_true_features[selected_feature].values, gen_traj_interp) / (\n",
    "    np.linalg.norm(this_obj_true_features[selected_feature].values)\n",
    "    * np.linalg.norm(gen_traj_interp)\n",
    ")\n",
    "\n",
    "l2_dist, cosine_sim, normd_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show L2 / cosine sim on all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_miss_true_traj_split = 0\n",
    "nb_orig_matches_miss = 0\n",
    "\n",
    "for true_object_id in tqdm(kept_true_ids):\n",
    "    plate_name = true_object_id.split(\"-\")[0]\n",
    "\n",
    "    matching_gen_id = true_cells_to_gen_cells_mapping[\n",
    "        re.sub(r\"-extracted_subtraj_\\d+\", \"\", true_object_id)\n",
    "    ][\"closest_gen_id\"]  # might not exist! (because the matching gen was itself a subtraj)\n",
    "    matching_obj_gen_features = gen_features.query(\"global_object_id == @matching_gen_id\")\n",
    "    assert matching_gen_id is not None\n",
    "\n",
    "    if len(matching_obj_gen_features) == 0:\n",
    "        nb_orig_matches_miss += 1\n",
    "\n",
    "        if \"-extracted_subtraj_\" in true_object_id:\n",
    "            nb_miss_true_traj_split += 1\n",
    "\n",
    "    elif len(matching_obj_gen_features) != NB_GEN_FRAMES[plate_name]:\n",
    "        raise RuntimeError(\n",
    "            f\"{len(matching_obj_gen_features)} timepoints for matching generated cell {matching_gen_id} of true id {true_object_id}, expected 50\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        assert matching_gen_id in gen_features[\"global_object_id\"].values\n",
    "\n",
    "print(\n",
    "    f\"\\n{nb_orig_matches_miss} in total out of {len(kept_true_ids)} kept true cells ({nb_orig_matches_miss / len(kept_true_ids) * 100:.1f}%) have no matches anymore in the generated data (because of generated trajectories splitting), and must be rematched\"\n",
    ")\n",
    "print(\n",
    "    f\"{nb_miss_true_traj_split} of these true trajs have also been split ({nb_miss_true_traj_split / nb_orig_matches_miss * 100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rematching strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "rematching_strategy = \"all-wheighted\"  # choose from: \"no\", \"random\", \"all\" TODO: add \"best\" options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_true_to_gen_traj(\n",
    "    these_plates_true_features: pd.DataFrame,\n",
    "    true_object_id: str,\n",
    "    true_cells_to_gen_cells_mapping: dict[str, dict[str, Any]],\n",
    "    gen_features: pd.DataFrame,\n",
    "    rematching_strategy: str,\n",
    "    selected_feature: str,\n",
    "    verbose: bool = False,\n",
    "    compute_weights_return_ids: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Appends to the lists given in args the `metrics` computed between the true trajectory corresponding\n",
    "    to `true_object_id` and the matching generated trajectory/ies in `gen_features`.\n",
    "    \"\"\"\n",
    "    # true features\n",
    "    this_obj_true_features = these_plates_true_features.query(\"global_object_id == @true_object_id\")\n",
    "    # generated features\n",
    "    matching_gen_id = true_cells_to_gen_cells_mapping[\n",
    "        re.sub(r\"-extracted_subtraj_\\d+\", \"\", true_object_id)\n",
    "    ][\"closest_gen_id\"]\n",
    "    # might not exist! (because the matching gen was itself a subtraj)\n",
    "    # -> so find new matches\n",
    "    if matching_gen_id not in gen_features[\"global_object_id\"].values:\n",
    "        split_matching_gen_ids = gen_features.loc[\n",
    "            gen_features[\"global_object_id\"].str.startswith(\n",
    "                matching_gen_id + \"-extracted_subtraj_\"\n",
    "            ),\n",
    "            \"global_object_id\",\n",
    "        ].unique()\n",
    "        split_matching_gen_ids = [str(id) for id in split_matching_gen_ids.copy()]\n",
    "        # choose one at random\n",
    "        if rematching_strategy == \"no\":\n",
    "            if verbose:\n",
    "                print(\"Skipping rematching for\", true_object_id)\n",
    "            return None\n",
    "        elif rematching_strategy == \"random\":\n",
    "            matching_gen_id = np.random.choice(split_matching_gen_ids)\n",
    "            if verbose:\n",
    "                print(\"Rematching for\", true_object_id, \"at random:\", matching_gen_id)\n",
    "            if compute_weights_return_ids:\n",
    "                weights = [1]\n",
    "                ids = [(true_object_id, matching_gen_id)]\n",
    "        elif rematching_strategy == \"all\":\n",
    "            matching_gen_id = split_matching_gen_ids\n",
    "            if verbose:\n",
    "                print(\"Rematching for\", true_object_id, \"at all matches:\", matching_gen_id)\n",
    "            if compute_weights_return_ids:\n",
    "                weights = [1] * len(matching_gen_id)\n",
    "                ids = [(true_object_id, gen_id) for gen_id in matching_gen_id]\n",
    "        elif rematching_strategy == \"all-wheighted\":\n",
    "            matching_gen_id = split_matching_gen_ids\n",
    "            if verbose:\n",
    "                print(\"Rematching for\", true_object_id, \"at all matches:\", matching_gen_id)\n",
    "            if compute_weights_return_ids:\n",
    "                weights = [1 / len(matching_gen_id)] * len(matching_gen_id)\n",
    "                ids = [(true_object_id, gen_id) for gen_id in matching_gen_id]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown rematching strategy: {rematching_strategy}\")\n",
    "    else:\n",
    "        # nothing special to do\n",
    "        if compute_weights_return_ids:\n",
    "            weights = [1]\n",
    "            ids = [(true_object_id, matching_gen_id)]\n",
    "\n",
    "    # compute distances\n",
    "    if not isinstance(matching_gen_id, list):\n",
    "        matching_gen_id = [matching_gen_id]\n",
    "\n",
    "    l2_dists = []\n",
    "    cosine_sims = []\n",
    "    normd_l2s = []\n",
    "    for gen_id in matching_gen_id:\n",
    "        matching_obj_gen_features = gen_features.query(\"global_object_id == @gen_id\")\n",
    "        _, l2_dist, cosine_sim, normed_l2 = compute_trajectory_comparison_metrics(\n",
    "            this_obj_true_features[selected_feature].values,\n",
    "            matching_obj_gen_features[selected_feature].values,\n",
    "        )\n",
    "        l2_dists.append(l2_dist)\n",
    "        cosine_sims.append(cosine_sim)\n",
    "        normd_l2s.append(normed_l2)\n",
    "\n",
    "    return (\n",
    "        l2_dists,\n",
    "        cosine_sims,\n",
    "        normd_l2s,\n",
    "        weights if compute_weights_return_ids else None,\n",
    "        ids if compute_weights_return_ids else None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute & plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cells_l2_dists = []\n",
    "all_cells_cosine_sims = []\n",
    "all_cells_normd_l2s = []\n",
    "all_cells_weights = []\n",
    "all_pairs_ids = []\n",
    "\n",
    "print(\"Using rematching strategy:\", rematching_strategy)\n",
    "print(\"Using feature:\", selected_feature)\n",
    "\n",
    "for true_object_id in tqdm(kept_true_ids):\n",
    "    res = compare_true_to_gen_traj(\n",
    "        these_plates_true_features,\n",
    "        true_object_id,\n",
    "        true_cells_to_gen_cells_mapping,\n",
    "        gen_features,\n",
    "        rematching_strategy,\n",
    "        selected_feature,\n",
    "        True,\n",
    "        True,\n",
    "    )\n",
    "    if res is not None:\n",
    "        l2_dists, cosine_sims, normd_l2s, weights, ids = res\n",
    "        all_pairs_ids.extend(ids)\n",
    "        all_cells_l2_dists.extend(l2_dists)\n",
    "        all_cells_cosine_sims.extend(cosine_sims)\n",
    "        all_cells_normd_l2s.extend(normd_l2s)\n",
    "        # if the true object is a subtraj, we need to re-weight *again* the matched generated trajectories,\n",
    "        # otherwise we are only correcting for the duplication of the generated trajectories,\n",
    "        # and *not* for that of the true ones!\n",
    "        if \"-extracted_subtraj_\" in true_object_id:  # that true object_id is a subtraj\n",
    "            nb_true_subtrajs_duplicated = len(\n",
    "                [\n",
    "                    id\n",
    "                    for id in kept_true_ids\n",
    "                    if id.startswith(re.sub(r\"-extracted_subtraj_\\d+\", \"\", true_object_id))\n",
    "                ]\n",
    "            )\n",
    "            # re-normalize weights so that the sum of the weights of all *true* subjtrajs equals one\n",
    "            weights = [w / nb_true_subtrajs_duplicated for w in weights]\n",
    "        all_cells_weights.extend(weights)\n",
    "\n",
    "assert (\n",
    "    len(all_cells_l2_dists)\n",
    "    == len(all_cells_cosine_sims)\n",
    "    == len(all_cells_normd_l2s)\n",
    "    == len(all_cells_weights)\n",
    "    == len(all_pairs_ids)\n",
    "), (\n",
    "    f\"Expected the same number of distances, weights, and pairs, but found {len(all_cells_l2_dists)}, {len(all_cells_cosine_sims)}, {len(all_cells_normd_l2s)}, {len(all_cells_weights)}, and {len(all_pairs_ids)}\"\n",
    ")\n",
    "if rematching_strategy != \"all-wheighted\":\n",
    "    assert all(w == 1 for w in all_cells_weights), (\n",
    "        f\"Expected all weights to be 1, but found {all_cells_weights}\"\n",
    "    )\n",
    "\n",
    "if rematching_strategy == \"all-wheighted\":\n",
    "    for true_object_id in kept_true_ids:\n",
    "        this_true_object_pairs_idxes = [\n",
    "            idx\n",
    "            for idx, (true_id, _) in enumerate(all_pairs_ids)\n",
    "            if true_id.startswith(re.sub(r\"-extracted_subtraj_\\d+\", \"\", true_object_id))\n",
    "        ]\n",
    "        assert sum(all_cells_weights[idx] for idx in this_true_object_pairs_idxes) == 1, (\n",
    "            f\"Expected the sum of weights for true object {true_object_id} to be 1, but found {sum(all_cells_weights[idx] for idx in this_true_object_pairs_idxes)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot 1: L2 distance\n",
    "mean_l2 = np.average(all_cells_l2_dists, weights=all_cells_weights)\n",
    "q1, med, q3 = np.quantile(\n",
    "    all_cells_l2_dists, [0.25, 0.5, 0.75], weights=all_cells_weights, method=\"inverted_cdf\"\n",
    ")\n",
    "iqr = q3 - q1\n",
    "whislo = max(q1 - 1.5 * iqr, np.min(all_cells_l2_dists))\n",
    "whishi = min(q3 + 1.5 * iqr, np.max(all_cells_l2_dists))\n",
    "sns.boxplot([whislo, q1, med, q3, whishi], showfliers=False, ax=axes[0])\n",
    "sns.swarmplot(all_cells_l2_dists, color=\".25\", size=2, ax=axes[0])\n",
    "axes[0].axhline(y=mean_l2, color=\"orange\", linestyle=\"-\")\n",
    "axes[0].text(\n",
    "    0.05,\n",
    "    0.95,\n",
    "    f\"Mean: {round(mean_l2, 1)}\\nMedian: {round(med, 1)}\",\n",
    "    transform=axes[0].transAxes,\n",
    "    verticalalignment=\"top\",\n",
    ")\n",
    "axes[0].set_ylabel(\"L2 distance\")\n",
    "axes[0].set_xlabel(selected_feature)\n",
    "axes[0].set_ylim(\n",
    "    0 - 0.05 * np.max(all_cells_l2_dists),\n",
    "    np.max(all_cells_l2_dists) + 0.05 * np.max(all_cells_l2_dists),\n",
    ")\n",
    "\n",
    "# Plot 2: Cosine similarity\n",
    "mean_cos = np.mean(all_cells_cosine_sims)\n",
    "q1, med, q3 = np.quantile(\n",
    "    all_cells_cosine_sims, [0.25, 0.5, 0.75], weights=all_cells_weights, method=\"inverted_cdf\"\n",
    ")\n",
    "iqr = q3 - q1\n",
    "whislo = max(q1 - 1.5 * iqr, np.min(all_cells_cosine_sims))\n",
    "whishi = min(q3 + 1.5 * iqr, np.max(all_cells_cosine_sims))\n",
    "sns.boxplot([whislo, q1, med, q3, whishi], showfliers=False, ax=axes[1])\n",
    "sns.swarmplot(all_cells_cosine_sims, color=\".25\", size=2, ax=axes[1])\n",
    "axes[1].axhline(y=mean_cos, color=\"orange\", linestyle=\"-\")\n",
    "axes[1].text(\n",
    "    0.05,\n",
    "    0.1,\n",
    "    f\"Mean: {round(mean_cos, 3)}\\nMedian: {round(med, 3)}\",\n",
    "    transform=axes[1].transAxes,\n",
    "    verticalalignment=\"top\",\n",
    ")\n",
    "axes[1].set_ylabel(\"Cosine similarity\")\n",
    "axes[1].set_xlabel(selected_feature)\n",
    "\n",
    "# Plot 3: Normalized L2 distance\n",
    "all_cells_normd_l2s_pct = np.array(all_cells_normd_l2s) * 100\n",
    "mean_normd = np.mean(all_cells_normd_l2s_pct)\n",
    "q1, med, q3 = np.quantile(\n",
    "    all_cells_normd_l2s_pct, [0.25, 0.5, 0.75], weights=all_cells_weights, method=\"inverted_cdf\"\n",
    ")\n",
    "iqr = q3 - q1\n",
    "whislo = max(q1 - 1.5 * iqr, np.min(all_cells_normd_l2s_pct))\n",
    "whishi = min(q3 + 1.5 * iqr, np.max(all_cells_normd_l2s_pct))\n",
    "sns.boxplot([whislo, q1, med, q3, whishi], showfliers=False, ax=axes[2])\n",
    "sns.swarmplot(all_cells_normd_l2s_pct, color=\".25\", size=2, ax=axes[2])\n",
    "axes[2].axhline(y=mean_normd, color=\"orange\", linestyle=\"-\")\n",
    "axes[2].text(\n",
    "    0.05,\n",
    "    0.95,\n",
    "    f\"Mean: {round(mean_normd, 2)}\\nMedian: {round(med, 2)}\",\n",
    "    transform=axes[2].transAxes,\n",
    "    verticalalignment=\"top\",\n",
    ")\n",
    "axes[2].set_ylabel(\"Normalized L2 distance (% of true traj)\")\n",
    "axes[2].set_xlabel(selected_feature)\n",
    "\n",
    "plt.suptitle(f\"Statistics on {len(all_cells_l2_dists)} trajectories from {len(plate_names)} fields\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show cosine sim and normalized L2 on all cells on all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(true_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"false\" features\n",
    "features_to_remove = [\n",
    "    \"file\",\n",
    "    \"Metadata_time\",\n",
    "    \"global_object_id\",\n",
    "    \"Location_Center_Z\",\n",
    "    \"ObjectNumber\",\n",
    "]\n",
    "features_to_remove += [\n",
    "    f\n",
    "    for f in true_features.columns\n",
    "    if f.startswith(\"TrackObjects_\")\n",
    "    or f.startswith(\"Metadata_\")\n",
    "    or f.startswith(\"Parent\")\n",
    "    or \"BoundingBox\" in f\n",
    "    or f == \"EulerNumber\"\n",
    "]\n",
    "features_to_compare = [f for f in true_features.columns if f not in features_to_remove]\n",
    "features_to_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rematching strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "rematching_strategy  # don't change here, weights must be the same as before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trajectory_comparison_metrics_for_this_feat(selected_feature: str):\n",
    "    \"\"\"\n",
    "    Compute metrics for the given `selected_feature`\n",
    "\n",
    "    TODO: factorize matching beforehand\n",
    "    TODO: parallelize along objects too\n",
    "    \"\"\"\n",
    "    this_feat_cosine_sims = []\n",
    "    this_feat_normd_l2s = []\n",
    "    for true_object_id in kept_true_ids:\n",
    "        res = compare_true_to_gen_traj(\n",
    "            these_plates_true_features,\n",
    "            true_object_id,\n",
    "            true_cells_to_gen_cells_mapping,\n",
    "            gen_features,\n",
    "            rematching_strategy,\n",
    "            selected_feature,\n",
    "        )\n",
    "        if res is not None:\n",
    "            _, cosine_sims, normd_l2s, _, _ = res\n",
    "            this_feat_cosine_sims.extend(cosine_sims)\n",
    "            this_feat_normd_l2s.extend(normd_l2s)\n",
    "\n",
    "    return {\"cosine_sim\": this_feat_cosine_sims, \"normd_l2\": this_feat_normd_l2s}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sims: \"dict[str, list]\" = dict.fromkeys(features_to_compare)\n",
    "normd_l2s: \"dict[str, list]\" = dict.fromkeys(features_to_compare)\n",
    "\n",
    "print(\"Using rematching strategy:\", rematching_strategy)\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    future_to_feature = {\n",
    "        executor.submit(\n",
    "            compute_trajectory_comparison_metrics_for_this_feat, selected_feature\n",
    "        ): selected_feature\n",
    "        for selected_feature in features_to_compare\n",
    "    }\n",
    "    for future in tqdm(as_completed(future_to_feature), total=len(future_to_feature)):\n",
    "        selected_feature = future_to_feature[future]\n",
    "        res = future.result()  # raises any exception\n",
    "        assert res is not None, f\"Expected a result for {selected_feature}, but got None\"\n",
    "        cosine_sims[selected_feature] = res[\"cosine_sim\"]\n",
    "        normd_l2s[selected_feature] = res[\"normd_l2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics plots helper funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_histograms(metrics_dict: dict[str, list[float]], weights: list[float]):\n",
    "    means = [np.average(v, weights=weights) for v in metrics_dict.values()]\n",
    "    medians = [\n",
    "        np.quantile(v, [0.5], weights=weights, method=\"inverted_cdf\")[0]\n",
    "        for v in metrics_dict.values()\n",
    "    ]\n",
    "\n",
    "    means_weights = np.ones_like(means) / len(means) * 100\n",
    "    medians_weights = np.ones_like(medians) / len(medians) * 100\n",
    "\n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), dpi=100)\n",
    "\n",
    "    # Plot histogram of means\n",
    "    hist = ax1.hist(\n",
    "        means,\n",
    "        bins=100,\n",
    "        range=(-1, 1),\n",
    "        weights=means_weights,\n",
    "        alpha=0.75,\n",
    "        color=\"blue\",\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "    ax1.set_title(\"Histogram of Mean Cosine Similarities\")\n",
    "    ax1.set_xlabel(\"Mean Cosine Similarity\")\n",
    "    ax1.set_ylabel(\"Number of Features (%)\")\n",
    "    ax1.grid(alpha=0.3)\n",
    "    ax1.vlines(\n",
    "        x=np.mean(means),\n",
    "        ymin=0,\n",
    "        ymax=hist[0].max(),\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"Mean of means\",\n",
    "    )\n",
    "    ax1.text(\n",
    "        x=np.mean(means) - 0.2,\n",
    "        y=hist[0].max() * 0.9,\n",
    "        s=f\"{np.mean(means):.2f}\",\n",
    "        color=\"red\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot histogram of medians\n",
    "    hist = ax2.hist(\n",
    "        medians,\n",
    "        bins=100,\n",
    "        range=(-1, 1),\n",
    "        weights=medians_weights,\n",
    "        alpha=0.75,\n",
    "        color=\"green\",\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "    ax2.set_title(\"Histogram of Median Cosine Similarities\")\n",
    "    ax2.set_xlabel(\"Median Cosine Similarity\")\n",
    "    ax2.set_ylabel(\"Number of Features (%)\")\n",
    "    ax2.grid(alpha=0.3)\n",
    "    ax2.vlines(\n",
    "        x=np.median(medians),\n",
    "        ymin=0,\n",
    "        ymax=hist[0].max(),\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=\"Median of medians\",\n",
    "    )\n",
    "\n",
    "    ax2.text(\n",
    "        x=np.median(medians) - 0.2,\n",
    "        y=hist[0].max() * 0.9,\n",
    "        s=f\"{np.median(medians):.2f}\",\n",
    "        color=\"red\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "    ax2.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metrics_boxplots(features_names: list[str], metrics: dict[str, list[float]]):\n",
    "    # Calculate the grid size for subplots\n",
    "    num_features = len(features_names)\n",
    "    num_cols = 10  # Number of columns in the subplot grid\n",
    "    num_rows = math.ceil(num_features / num_cols)  # Calculate number of rows needed\n",
    "    print(\"num features\", num_features)\n",
    "    print(\"num rows\", num_rows)\n",
    "    print(\"num cols\", num_cols)\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5 * num_rows), dpi=150)\n",
    "    axes = axes.flatten() if num_features > 1 else [axes]\n",
    "\n",
    "    # Create a boxplot and swarmplot for each feature\n",
    "    for i, feature in enumerate(tqdm(features_names)):\n",
    "        try:\n",
    "            sns.boxplot(y=metrics[feature], ax=axes[i], showfliers=False)\n",
    "            sns.stripplot(y=metrics[feature], ax=axes[i], color=\".25\", size=3)\n",
    "            axes[i].set_ylabel(feature)\n",
    "            axes[i].set_ylim(-1, 1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing feature {feature}: {e}\")\n",
    "            axes[i].set_visible(False)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(num_features, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.suptitle(\"Feature Cosine Similarities ordered by mean\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.96)  # Adjust to make room for suptitle\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throw NaN cosine features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_features = []\n",
    "\n",
    "for k, v in cosine_sims.items():\n",
    "    print(f\"    {k}: {np.mean(v):.3f} ± {np.std(v):.3f}\")\n",
    "    if np.isnan(np.mean(v)):\n",
    "        print(f\"WARNING!!! => feature {k} has NaN mean\")\n",
    "        nan_features.append(k)\n",
    "\n",
    "print(\"NaN features:\", nan_features)\n",
    "nan_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_nan_features = [k for k in cosine_sims.keys() if k not in nan_features]\n",
    "print(f\"Kept {len(non_nan_features)} non-nan features out of {len(cosine_sims)}\")\n",
    "\n",
    "non_nan_cosine_sims = {k: v for k, v in cosine_sims.items() if k in non_nan_features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_histograms(non_nan_cosine_sims, all_cells_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_order_by_mean_cossim = sorted(\n",
    "    non_nan_cosine_sims, key=lambda f: np.mean(cosine_sims[f]), reverse=True\n",
    ")\n",
    "features_order_by_mean_cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_boxplots(features_order_by_mean_cossim, non_nan_cosine_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized L2 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throw NaN normalized L2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_features = []\n",
    "\n",
    "for k, v in normd_l2s.items():\n",
    "    print(f\"    {k}: {np.mean(v):.3f} ± {np.std(v):.3f}\")\n",
    "    if np.isnan(np.mean(v)):\n",
    "        print(f\"WARNING!!! => feature {k} has NaN mean\")\n",
    "        nan_features.append(k)\n",
    "\n",
    "print(\"NaN features:\", nan_features)\n",
    "nan_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_nan_features = [k for k in normd_l2s.keys() if k not in nan_features]\n",
    "print(f\"Kept {len(non_nan_features)} non-nan features out of {len(normd_l2s)}\")\n",
    "\n",
    "non_nan_normd_l2s = {k: v for k, v in normd_l2s.items() if k in non_nan_features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized L2 Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [np.mean(v) for v in non_nan_normd_l2s.values()]\n",
    "medians = [np.median(v) for v in non_nan_normd_l2s.values()]\n",
    "\n",
    "means_weights = np.ones_like(means) / len(means) * 100\n",
    "medians_weights = np.ones_like(medians) / len(medians) * 100\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), dpi=100)\n",
    "\n",
    "# Plot histogram of means\n",
    "hist = ax1.hist(\n",
    "    np.array(means) * 100,\n",
    "    bins=100,\n",
    "    range=(0, np.max(means) * 100),\n",
    "    weights=means_weights,\n",
    "    alpha=0.75,\n",
    "    color=\"blue\",\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "ax1.set_title(\"Histogram of Mean Normalized L2 Norm\")\n",
    "ax1.set_xlabel(\"Mean Normalized L2 Norm (% of true traj)\")\n",
    "ax1.set_ylabel(\"Number of Features (%)\")\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.vlines(\n",
    "    x=np.mean(means) * 100,\n",
    "    ymin=0,\n",
    "    ymax=hist[0].max(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Mean of means\",\n",
    ")\n",
    "ax1.text(\n",
    "    x=np.mean(means) * 100 - 2,\n",
    "    y=hist[0].max() * 0.9,\n",
    "    s=f\"{np.mean(means) * 100:.2f}\",\n",
    "    color=\"red\",\n",
    "    fontsize=10,\n",
    ")\n",
    "ax1.legend()\n",
    "\n",
    "# Plot histogram of medians\n",
    "hist = ax2.hist(\n",
    "    np.array(medians) * 100,\n",
    "    bins=100,\n",
    "    range=(0, np.max(medians) * 100),\n",
    "    weights=medians_weights,\n",
    "    alpha=0.75,\n",
    "    color=\"green\",\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "ax2.set_title(\"Histogram of Median Normalized L2 Norm\")\n",
    "ax2.set_xlabel(\"Median Normalized L2 Norm (% of true traj)\")\n",
    "ax2.set_ylabel(\"Number of Features (%)\")\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.vlines(\n",
    "    x=np.median(medians) * 100,\n",
    "    ymin=0,\n",
    "    ymax=hist[0].max(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Median of medians\",\n",
    ")\n",
    "\n",
    "ax2.text(\n",
    "    x=np.median(medians) * 100 - 2,\n",
    "    y=hist[0].max() * 0.9,\n",
    "    s=f\"{np.median(medians) * 100:.2f}\",\n",
    "    color=\"red\",\n",
    "    fontsize=10,\n",
    ")\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "redo the hist of mean but plotting only between 0 and 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [np.mean(v) for v in non_nan_normd_l2s.values()]\n",
    "medians = [np.median(v) for v in non_nan_normd_l2s.values()]\n",
    "\n",
    "means_weights = np.ones_like(means) / len(means) * 100\n",
    "medians_weights = np.ones_like(medians) / len(medians) * 100\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5), dpi=100)\n",
    "# Plot histogram of means\n",
    "plt.hist(\n",
    "    np.array(means) * 100,\n",
    "    bins=100,\n",
    "    range=(0, 100),\n",
    "    weights=means_weights,\n",
    "    alpha=0.75,\n",
    "    color=\"blue\",\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "plt.title(\"Histogram of Mean Normalized L2 Norm\")\n",
    "plt.xlabel(\"Mean Normalized L2 Norm (% of true traj)\")\n",
    "plt.ylabel(\"Number of Features (%)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5), dpi=100)\n",
    "# Plot histogram of means\n",
    "plt.hist(\n",
    "    np.array(means) * 100,\n",
    "    bins=100,\n",
    "    range=(0, 1000),\n",
    "    weights=means_weights,\n",
    "    alpha=0.75,\n",
    "    color=\"blue\",\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "plt.title(\"Histogram of Mean Normalized L2 Norm\")\n",
    "plt.xlabel(\"Mean Normalized L2 Norm (% of true traj)\")\n",
    "plt.ylabel(\"Number of Features (%)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# Use the same data from your original code\n",
    "means_data = np.array(means) * 100\n",
    "\n",
    "# Create the first cumulative histogram (0-100 range)\n",
    "fig1 = px.histogram(\n",
    "    means_data,\n",
    "    nbins=5000,\n",
    "    range_x=[0, 500],\n",
    "    cumulative=True,\n",
    "    histnorm=\"percent\",\n",
    "    title=\"Cumulative Histogram of Mean Normalized L2 Norm\",\n",
    "    opacity=0.5,\n",
    ")\n",
    "\n",
    "fig1.update_layout(\n",
    "    xaxis_title=\"Mean Normalized L2 Norm (% of true traj)\",\n",
    "    yaxis_title=\"Cumulative Percentage of Features\",\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_order_by_mean_normd_l2 = sorted(non_nan_normd_l2s, key=lambda f: np.mean(normd_l2s[f]))\n",
    "features_order_by_mean_normd_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the grid size for subplots\n",
    "num_features = len(features_order_by_mean_normd_l2)\n",
    "num_cols = 10  # Number of columns in the subplot grid\n",
    "num_rows = math.ceil(num_features / num_cols)  # Calculate number of rows needed\n",
    "print(\"num features\", num_features)\n",
    "print(\"num rows\", num_rows)\n",
    "print(\"num cols\", num_cols)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 5 * num_rows), dpi=200)\n",
    "axes = axes.flatten() if num_features > 1 else [axes]\n",
    "\n",
    "# Create a boxplot and swarmplot for each feature\n",
    "for i, feature in enumerate(tqdm(features_order_by_mean_normd_l2)):\n",
    "    try:\n",
    "        sns.boxplot(y=np.array(normd_l2s[feature]) * 100, ax=axes[i], showfliers=False)\n",
    "        sns.stripplot(y=np.array(normd_l2s[feature]) * 100, ax=axes[i], color=\".25\", size=3)\n",
    "        axes[i].set_ylabel(feature + \" (%)\")\n",
    "        axes[i].set_ylim(0, np.max(normd_l2s[feature]) * 100)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing feature {feature}: {e}\")\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(num_features, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle(\n",
    "    f\"Feature Normalized L2 Metrics (% of true traj) ordered by mean - {plate_name}\", fontsize=16\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.96)  # Adjust to make room for suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "these_plates_true_features[features_to_compare]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "correlation_matrix = these_plates_true_features[features_to_compare].corr()\n",
    "print(f\"Computed correlation matrix of shape {correlation_matrix.shape}\")\n",
    "\n",
    "# drop Nan values\n",
    "correlation_matrix = correlation_matrix.dropna(axis=1, how=\"all\")\n",
    "correlation_matrix = correlation_matrix.dropna(axis=0, how=\"all\")\n",
    "print(f\"After dropping NaN values, correlation matrix shape is {correlation_matrix.shape}\")\n",
    "\n",
    "# Create a clustered heatmap visualization\n",
    "plt.figure(figsize=(25, 20))\n",
    "# Use hierarchical clustering to group correlated features\n",
    "linkage = scipy.cluster.hierarchy.linkage(correlation_matrix, method=\"ward\")\n",
    "dendro = scipy.cluster.hierarchy.dendrogram(\n",
    "    linkage, labels=correlation_matrix.columns, no_plot=True\n",
    ")\n",
    "reordered_idx = dendro[\"leaves\"]\n",
    "reordered_corr = correlation_matrix.iloc[reordered_idx, reordered_idx]\n",
    "\n",
    "# Find highly correlated pairs\n",
    "corr_threshold = 0.95  # Correlation threshold\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > corr_threshold:\n",
    "            high_corr_pairs.append(\n",
    "                (\n",
    "                    correlation_matrix.columns[i],\n",
    "                    correlation_matrix.columns[j],\n",
    "                    correlation_matrix.iloc[i, j],\n",
    "                )\n",
    "            )\n",
    "\n",
    "# Sort by absolute correlation value\n",
    "high_corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "print(\n",
    "    f\"Found {len(high_corr_pairs)} feature pairs with absolute correlation > {corr_threshold} ({round(len(high_corr_pairs) / len(correlation_matrix) * 100)}% of total)\"\n",
    ")\n",
    "for feature1, feature2, corr in high_corr_pairs:\n",
    "    print(f\"    {feature1} ~ {feature2}: {corr:.3f}\")\n",
    "\n",
    "# Plot the heatmap\n",
    "mask = np.triu(np.ones_like(reordered_corr))  # Keep only lower triangle\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(\n",
    "    reordered_corr,\n",
    "    mask=mask,\n",
    "    cmap=cmap,\n",
    "    vmax=1,\n",
    "    vmin=-1,\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.005,\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    ")\n",
    "# Make colorbar tick labels bigger\n",
    "cbar = plt.gcf().axes[-1]  # Get the colorbar axes\n",
    "cbar.tick_params(labelsize=14)  # Set tick label size\n",
    "\n",
    "highly_correlated_features = set()\n",
    "for feature1, feature2, _ in high_corr_pairs:\n",
    "    highly_correlated_features.add(feature1)\n",
    "    highly_correlated_features.add(feature2)\n",
    "# Get the tick labels\n",
    "ax = plt.gca()\n",
    "xticklabels = ax.get_xticklabels()\n",
    "yticklabels = ax.get_yticklabels()\n",
    "\n",
    "# Make labels bold for highly correlated features\n",
    "for label in xticklabels:\n",
    "    if label.get_text() in highly_correlated_features:\n",
    "        label.set_fontweight(\"bold\")\n",
    "\n",
    "for label in yticklabels:\n",
    "    if label.get_text() in highly_correlated_features:\n",
    "        label.set_fontweight(\"bold\")\n",
    "\n",
    "# Apply changes\n",
    "plt.draw()\n",
    "\n",
    "plt.title(\"Feature Correlation Matrix (Clustered)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot histograms of uncorrelated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_indep_features = set(non_nan_cosine_sims.keys())\n",
    "\n",
    "for f1, f2, _ in high_corr_pairs:\n",
    "    if f1 not in kept_indep_features or f2 not in kept_indep_features:\n",
    "        # either f1 or f2 was already removed, no need to do anything from this pair\n",
    "        continue\n",
    "    else:\n",
    "        # we need to remove one of the two features\n",
    "        f_rm = random.choice([f1, f2])\n",
    "        kept_indep_features.remove(f_rm)\n",
    "        print(f\"Keeping {f1 if f_rm == f2 else f2} and removing {f_rm} from kept features\")\n",
    "\n",
    "print(\n",
    "    f\"Kept {len(kept_indep_features)} independent features out of {len(features_to_compare)} ({len(kept_indep_features) / len(features_to_compare) * 100:.0f}%)\"\n",
    ")\n",
    "\n",
    "# check\n",
    "tmp_check = []\n",
    "tmp_filtered_corr_mat = correlation_matrix.loc[list(kept_indep_features), list(kept_indep_features)]\n",
    "for i in range(len(tmp_filtered_corr_mat.columns)):\n",
    "    for j in range(i):\n",
    "        assert abs(tmp_filtered_corr_mat.iloc[i, j]) <= corr_threshold, (\n",
    "            f\"{tmp_filtered_corr_mat.iloc[i, j]} > {corr_threshold} for {tmp_filtered_corr_mat.columns[i]} and {tmp_filtered_corr_mat.columns[j]}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_histograms(\n",
    "    {k: v for k, v in non_nan_cosine_sims.items() if k in kept_indep_features}, all_cells_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple vs non-simple objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First show some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_nonsimple_cells = 0\n",
    "nb_simple_cells = 0\n",
    "\n",
    "for true_id in kept_true_ids:\n",
    "    if \"-extracted_subtraj_\" in true_id:\n",
    "        assert (\n",
    "            \"not_simple_true_object\"\n",
    "            in true_cells_to_gen_cells_mapping[\"-\".join(true_id.split(\"-\")[:2])][\"object_type\"]\n",
    "        )\n",
    "        nb_nonsimple_cells += 1\n",
    "    else:\n",
    "        nb_simple_cells += 1\n",
    "\n",
    "print(\n",
    "    f\"{nb_simple_cells} simple true cells ({nb_simple_cells / len(kept_true_ids) * 100:.1f}%) and {nb_nonsimple_cells} nonsimple true cells ({nb_nonsimple_cells / len(kept_true_ids) * 100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate simple and non-simple objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_pairs_idxes = []  # pairs of simple objects\n",
    "nonsimple_pairs_idxes = []  # pairs of non-simple objects (at least one)\n",
    "\n",
    "for idx, (true_id, gen_id) in enumerate(all_pairs_ids):\n",
    "    if \"-extracted_subtraj_\" not in true_id and \"-extracted_subtraj_\" not in gen_id:\n",
    "        simple_pairs_idxes.append(idx)\n",
    "    else:\n",
    "        nonsimple_pairs_idxes.append(idx)\n",
    "\n",
    "print(\n",
    "    f\"{len(simple_pairs_idxes)} simple pairs ({len(simple_pairs_idxes) / len(all_pairs_ids) * 100:.1f}%) and {len(nonsimple_pairs_idxes)} nonsimple pairs ({len(nonsimple_pairs_idxes) / len(all_pairs_ids) * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"In original data before duplication *and filtering*, we had {object_type_counts['not_simple_true_object']} nonsimple cells (out of: we didn't keep track!)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot histograms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_nan_cosine_sims_simple_cells = {\n",
    "    k: np.array(v)[simple_pairs_idxes] for k, v in non_nan_cosine_sims.items()\n",
    "}\n",
    "non_nan_cosine_sims_nonsimple_cells = {\n",
    "    k: np.array(v)[nonsimple_pairs_idxes] for k, v in non_nan_cosine_sims.items()\n",
    "}\n",
    "\n",
    "print(\"Non simple pairs:\")\n",
    "plot_metrics_histograms(non_nan_cosine_sims_nonsimple_cells, [1] * len(nonsimple_pairs_idxes))\n",
    "print(\"Simple pairs:\")\n",
    "plot_metrics_histograms(non_nan_cosine_sims_simple_cells, [1] * len(simple_pairs_idxes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot histograms of uncorrelated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_nan_cosine_sims_simple_cells_uncorr_feats = {\n",
    "    k: np.array(v)[simple_pairs_idxes]\n",
    "    for k, v in non_nan_cosine_sims.items()\n",
    "    if k in kept_indep_features\n",
    "}\n",
    "non_nan_cosine_sims_nonsimple_cells_uncorr_feats = {\n",
    "    k: np.array(v)[nonsimple_pairs_idxes]\n",
    "    for k, v in non_nan_cosine_sims.items()\n",
    "    if k in kept_indep_features\n",
    "}\n",
    "\n",
    "print(\"Non simple pairs:\")\n",
    "plot_metrics_histograms(\n",
    "    non_nan_cosine_sims_nonsimple_cells_uncorr_feats, [1] * len(nonsimple_pairs_idxes)\n",
    ")\n",
    "print(\"Simple pairs:\")\n",
    "plot_metrics_histograms(\n",
    "    non_nan_cosine_sims_simple_cells_uncorr_feats, [1] * len(simple_pairs_idxes)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GaussianProxy",
   "language": "python",
   "name": "gaussian_proxy"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
